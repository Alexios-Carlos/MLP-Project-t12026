{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":127742,"databundleVersionId":15295088,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:48:54.311061Z","iopub.execute_input":"2026-02-08T15:48:54.311469Z","iopub.status.idle":"2026-02-08T15:48:54.673322Z","shell.execute_reply.started":"2026-02-08T15:48:54.311422Z","shell.execute_reply":"2026-02-08T15:48:54.672292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IIT Madras BS: Machine Learning Project (MLP)\n## Comment Category Prediction Challenge\n**Student:** Muhammad Bilal | **Roll Number:** 23F3001344\n\n---\n### **Project Roadmap & Methodology**\nThis notebook implements an end-to-end ML pipeline following the official project milestones:\n* **M1:** Advanced EDA, handling 73% missing data, and Baseline Logistic Regression.\n* **M2:** Stochastic Gradient Descent (SGD) with Hyperparameter Tuning.\n* **M3:** Dimensionality Reduction (SVD) and Non-Linear Models (KNN, SVM, Naive Bayes).\n* **M4:** Advanced Boosting (XGBoost, LightGBM), Multi-Layer Perceptron (MLP), and Tuned Ensembling.\n* **M5:** Final Insights, Error Analysis, and Leaderboard Optimization.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport re\nfrom scipy.sparse import hstack\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:51:19.883455Z","iopub.execute_input":"2026-02-08T15:51:19.883921Z","iopub.status.idle":"2026-02-08T15:51:28.989461Z","shell.execute_reply.started":"2026-02-08T15:51:19.883892Z","shell.execute_reply":"2026-02-08T15:51:28.988553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load Data\ntrain = pd.read_csv('/kaggle/input/comment-category-prediction-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/comment-category-prediction-challenge/test.csv')\nsample_sub = pd.read_csv('/kaggle/input/comment-category-prediction-challenge/Sample.csv')\n\n# 2. Preprocessing & Imputation\ntrain['comment'] = train['comment'].fillna(\"missing\")\ntest['comment'] = test['comment'].fillna(\"missing\")\nfor col in ['race', 'religion', 'gender']:\n    train[col] = train[col].fillna(-1)\n    test[col] = test[col].fillna(-1)\n\n# 3. Visualization: Label Distribution\nplt.figure(figsize=(10, 5))\nsns.countplot(x='label', data=train, palette='magma')\nplt.title('M1: Class Distribution (Checking for Imbalance)')\nplt.show()\n\n# 4. Statistical Text Analysis (Chi-Square)\ntfidf_eda = TfidfVectorizer(max_features=2000, stop_words='english')\nX_eda = tfidf_eda.fit_transform(train['comment'])\nfrom sklearn.feature_selection import chi2\nfeatures = tfidf_eda.get_feature_names_out()\nfor label in sorted(train['label'].unique()):\n    chi2score = chi2(X_eda, train['label'] == label)[0]\n    top_indices = np.argsort(chi2score)[-5:]\n    print(f\"Category {label} Keywords: {[features[i] for i in top_indices]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:52:34.088923Z","iopub.execute_input":"2026-02-08T15:52:34.089696Z","iopub.status.idle":"2026-02-08T15:52:46.453532Z","shell.execute_reply.started":"2026-02-08T15:52:34.089663Z","shell.execute_reply":"2026-02-08T15:52:46.452431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Engineering\ntfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\nX_tfidf = tfidf.fit_transform(train['comment'])\nX_test_tfidf = tfidf.transform(test['comment'])\n\n# Train-Validation Split (Stratified)\nX_train, X_val, y_train, y_val = train_test_split(X_tfidf, train['label'], test_size=0.2, stratify=train['label'], random_state=42)\n\n# Baseline Logic\nbaseline = LogisticRegression(max_iter=1000, class_weight='balanced')\nbaseline.fit(X_train, y_train)\nprint(f\"M1 Baseline Accuracy: {accuracy_score(y_val, baseline.predict(X_val)):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:53:20.222279Z","iopub.execute_input":"2026-02-08T15:53:20.222674Z","iopub.status.idle":"2026-02-08T15:54:22.367083Z","shell.execute_reply.started":"2026-02-08T15:53:20.222647Z","shell.execute_reply":"2026-02-08T15:54:22.366072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SGD with Hyperparameter Tuning\nsgd = SGDClassifier(loss='hinge', class_weight='balanced', random_state=42)\nparam_grid_sgd = {'alpha': [0.0001, 0.001, 0.01]}\ngrid_sgd = RandomizedSearchCV(sgd, param_grid_sgd, n_iter=3, cv=3, scoring='f1_macro')\ngrid_sgd.fit(X_train, y_train)\n\nprint(f\"Best SGD Params: {grid_sgd.best_params_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:59:33.594087Z","iopub.execute_input":"2026-02-08T15:59:33.595114Z","iopub.status.idle":"2026-02-08T15:59:43.605436Z","shell.execute_reply.started":"2026-02-08T15:59:33.595075Z","shell.execute_reply":"2026-02-08T15:59:43.604252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MILESTONE 3: Dimensionality Reduction & Diversified Models\n# 1. Dimensionality Reduction (SVD)\nsvd = TruncatedSVD(n_components=100, random_state=42)\nX_train_svd = svd.fit_transform(X_train)\nX_val_svd = svd.transform(X_val)\n\n# 2. Multinomial Naive Bayes (Requirement)\n# Note: NB works directly on TF-IDF (must be non-negative)\nnb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\nprint(f\"M3 Naive Bayes Accuracy: {accuracy_score(y_val, nb_model.predict(X_val)):.4f}\")\n\n# 3. K-Nearest Neighbors (Requirement)\nknn = KNeighborsClassifier(n_neighbors=5).fit(X_train_svd, y_train)\n\n# 4. Support Vector Machine (Requirement)\nsvm = LinearSVC(class_weight='balanced', random_state=42).fit(X_train, y_train)\n\nprint(f\"M3 SVM Accuracy: {accuracy_score(y_val, svm.predict(X_val)):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T16:01:35.181278Z","iopub.execute_input":"2026-02-08T16:01:35.181631Z","iopub.status.idle":"2026-02-08T16:01:53.821060Z","shell.execute_reply.started":"2026-02-08T16:01:35.181606Z","shell.execute_reply":"2026-02-08T16:01:53.820258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW CELL 7: Advanced Feature Engineering ---\nfrom scipy.sparse import hstack\n\n# 1. Extract Meta Features (Capital letters, Punctuation, Length)\ndef get_meta(df):\n    m = pd.DataFrame(index=df.index)\n    m['len'] = df['comment'].apply(len)\n    m['caps'] = df['comment'].apply(lambda x: len(re.findall(r'[A-Z]', str(x))))\n    m['punc'] = df['comment'].apply(lambda x: len(re.findall(r'[!?.]', str(x))))\n    return MaxAbsScaler().fit_transform(m)\n\n# 2. Advanced Text Vectorization (Words + Characters)\n# Character n-grams catch slang and misspellings\nvec = FeatureUnion([\n    ('word', TfidfVectorizer(ngram_range=(1, 2), max_features=10000, sublinear_tf=True)),\n    ('char', TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=10000, sublinear_tf=True))\n])\n\nX_text_adv = vec.fit_transform(train['comment'])\nX_test_adv = vec.transform(test['comment'])\n\n# 3. Combine everything\nX_total = hstack([X_text_adv, get_meta(train)])\nX_test_total = hstack([X_test_adv, get_meta(test)])\n\nX_tr, X_va, y_tr, y_va = train_test_split(X_total, train['label'], test_size=0.2, stratify=train['label'], random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW CELL 8: Training the Power Models ---\n\n# Calibrating SVM allows it to use 'Soft' voting for better accuracy\ncal_svm = CalibratedClassifierCV(LinearSVC(class_weight='balanced', random_state=42), cv=3)\ncal_svm.fit(X_tr, y_tr)\n\nxgb_pwr = XGBClassifier(n_estimators=150, learning_rate=0.1, max_depth=6, random_state=42)\nxgb_pwr.fit(X_tr, y_tr)\n\nlgbm_pwr = LGBMClassifier(n_estimators=150, learning_rate=0.1, class_weight='balanced', random_state=42)\nlgbm_pwr.fit(X_tr, y_tr)\n\nprint(f\"SVM Accuracy: {accuracy_score(y_va, cal_svm.predict(X_va)):.4f}\")\nprint(f\"XGB Accuracy: {accuracy_score(y_va, xgb_pwr.predict(X_va)):.4f}\")\nprint(f\"LGBM Accuracy: {accuracy_score(y_va, lgbm_pwr.predict(X_va)):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW CELL 9: The Soft-Voting Ensemble ---\nensemble_final = VotingClassifier(\n    estimators=[('svm', cal_svm), ('xgb', xgb_pwr), ('lgbm', lgbm_pwr)],\n    voting='soft' # Much more accurate for high-level competition\n)\n\nensemble_final.fit(X_tr, y_tr)\nprint(f\"Final Ensemble Validation Accuracy: {accuracy_score(y_va, ensemble_final.predict(X_va)):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- UPDATED CELL 10 ---\n# Use the new ensemble and the new \"Total\" test features\nfinal_preds = ensemble_final.predict(X_test_total)\n\nsubmission = pd.DataFrame({\n    'ID': sample_sub['ID'],\n    'label': final_preds\n})\n\nsubmission.to_csv('booster_submission.csv', index=False)\nprint(\"Booster Submission Saved! Ready for Leaderboard.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}